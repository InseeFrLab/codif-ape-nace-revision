apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: run-relabel-naf08-to-naf25
spec:
  serviceAccountName: workflow
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 64Gi
  entrypoint: main
  arguments:
    parameters:
      - name: params-conf-list
        value: '[
          {"LLM_NAME": "mistralai/Mistral-Small-3.1-24B-Instruct-2503", "METHOD": "rag"},
          {"LLM_NAME": "Qwen/Qwen2.5-32B-Instruct", "METHOD": "rag"},
          {"LLM_NAME": "google/gemma-3-27b-it", "METHOD": "rag"},
          {"LLM_NAME": "mistralai/Mistral-Small-3.1-24B-Instruct-2503", "METHOD": "cag"},
          {"LLM_NAME": "Qwen/Qwen2.5-32B-Instruct", "METHOD": "cag"},
          {"LLM_NAME": "google/gemma-3-27b-it", "METHOD": "cag"}
          ]'

  templates:
    - name: main
      dag:
        tasks:
          - name: relabel-naf08-to-naf25
            template: run-relabel
            arguments:
              parameters:
                - name: LLM_NAME
                  value: "{{item.LLM_NAME}}"
                - name: METHOD
                  value: "{{item.METHOD}}"
            withParam: "{{workflow.parameters.params-conf-list}}"
    - name: run-relabel
      inputs:
        parameters:
          - name: LLM_NAME
          - name: METHOD
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.com/gpu.product
                    operator: In
                    values:
                      - NVIDIA-H100-NVL
                      - NVIDIA-H100-PCIe
      container:
        image: inseefrlab/onyxia-vscode-pytorch:py3.12.6-gpu
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            nvidia.com/gpu: 1
        command: ["/bin/bash", -c]
        args:
          - |
            git clone https://github.com/InseeFrLab/codif-ape-nace-revision.git
            cd codif-ape-nace-revision/
            pip install uv
            uv pip install -r pyproject.toml

            export MC_HOST_s3=https://$AWS_ACCESS_KEY_ID:$AWS_SECRET_ACCESS_KEY@$AWS_S3_ENDPOINT
            ./bash/fetch_model_s3.sh {{inputs.parameters.LLM_NAME}} $LOCAL_PATH

            if [ "{{inputs.parameters.METHOD}}" != "cag" ]; then
              uv run encode-ambiguous-{{inputs.parameters.METHOD}}.py --experiment_name NACE2025_DATASET2 --llm_name {{inputs.parameters.LLM_NAME}} --prompts_from_file 1
            else
              uv run encode-ambiguous-{{inputs.parameters.METHOD}}.py --experiment_name NACE2025_DATASET2 --llm_name {{inputs.parameters.LLM_NAME}}
            fi
        volumeMounts:
          - mountPath: /dev/shm
            name: dshm
        env:
          # env var for s3 connexion
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: my-s3-creds
                key: accessKey
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: my-s3-creds
                key: secretKey
          - name: AWS_DEFAULT_REGION
            value: us-east-1
          - name: AWS_S3_ENDPOINT
            value: minio.lab.sspcloud.fr
          - name: QDRANT_API_KEY
            valueFrom:
              secretKeyRef:
                name: qdrant-apikey
                key: api-key
          - name: MLFLOW_S3_ENDPOINT_URL
            value: https://minio.lab.sspcloud.fr
          - name: MLFLOW_TRACKING_URI
            value: https://projet-ape-mlflow.user.lab.sspcloud.fr/
          - name: LOCAL_PATH
            value: /home/onyxia/.cache/huggingface/hub
          - name: VLLM_USE_V1
            value: "1"
          - name: VLLM_USE_FLASHINFER_SAMPLER
            value: "0"
          - name: TORCH_CUDA_ARCH_LIST
            value: "9.0"
